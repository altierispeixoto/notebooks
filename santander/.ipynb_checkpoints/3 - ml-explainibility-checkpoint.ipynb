{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " <a id=\"5\"></a> <br>\n",
    "# 5- Machine Learning Explainability for Santander\n",
    "In this section, I want to try extract insights from models with the help of this excellent [**Course**](https://www.kaggle.com/learn/machine-learning-explainability) in Kaggle.\n",
    "The Goal behind of ML Explainability for Santander is:\n",
    "1. All features are senseless named.(var_1, var2,...) but certainly the importance of each one is different!\n",
    "1. Extract insights from models.\n",
    "1. Find the most inmortant feature in models.\n",
    "1. Affect of each feature on the model's predictions.\n",
    "<img src='http://s8.picofile.com/file/8353215168/ML_Explain.png'>\n",
    "\n",
    "As you can see from the above, we will refer to three important and practical concepts in this section and try to explain each of them in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " <a id=\"51\"></a> <br>\n",
    "## 5-1 Permutation Importance\n",
    " In this section we will answer following question:\n",
    " 1. What features have the biggest impact on predictions?\n",
    " 1. how to extract insights from models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare our data for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=[\"target\",\"ID_code\"]\n",
    "X = train.drop(cols,axis=1)\n",
    "y = train[\"target\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test  = test.drop(\"ID_code\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create  a sample model to calculate which feature are more important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "rfc_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " <a id=\"52\"></a> <br>\n",
    "## 5-2 How to calculate and show importances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here is how to calculate and show importances with the [eli5](https://eli5.readthedocs.io/en/latest/) library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "perm = PermutationImportance(rfc_model, random_state=1).fit(val_X, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(perm, feature_names = val_X.columns.tolist(), top=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id=\"53\"></a> <br>\n",
    "## 5-3 What can be inferred from the above?\n",
    "1. As you move down the top of the graph, the importance of the feature decreases.\n",
    "1. The features that are shown in green indicate that they have a positive impact on our prediction\n",
    "1. The features that are shown in white indicate that they have no effect on our prediction\n",
    "1. The features shown in red indicate that they have a negative impact on our prediction\n",
    "1.  The most important feature was **Var_110**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id=\"54\"></a> <br>\n",
    "## 5-4 Partial Dependence Plots\n",
    "While **feature importance** shows what **variables** most affect predictions, **partial dependence** plots show how a feature affects predictions.[6][7]\n",
    "and partial dependence plots are calculated after a model has been fit. [partial-plots](https://www.kaggle.com/dansbecker/partial-plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "tree_model = DecisionTreeClassifier(random_state=0, max_depth=5, min_samples_split=5).fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "For the sake of explanation, I use a Decision Tree which you can see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [c for c in train.columns if c not in ['ID_code', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import graphviz\n",
    "tree_graph = tree.export_graphviz(tree_model, out_file=None, feature_names=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphviz.Source(tree_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "As guidance to read the tree:\n",
    "\n",
    "1. Leaves with children show their splitting criterion on the top\n",
    "1. The pair of values at the bottom show the count of True values and False values for the target respectively, of data points in that node of the tree.\n",
    "><font color=\"red\"><b>Note: </b></font>\n",
    "Yes **Var_81** are more effective on our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id=\"55\"></a> <br>\n",
    "## 5-5  Partial Dependence Plot\n",
    "In this section, we see the impact of the main variables discovered in the previous sections by using the [pdpbox](https://pdpbox.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from pdpbox import pdp, get_dataset, info_plots\n",
    "\n",
    "# Create the data that we will plot\n",
    "pdp_goals = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=features, feature='var_81')\n",
    "\n",
    "# plot it\n",
    "pdp.pdp_plot(pdp_goals, 'var_81')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id=\"56\"></a> <br>\n",
    "## 5-6 Chart analysis\n",
    "1. The y axis is interpreted as change in the prediction from what it would be predicted at the baseline or leftmost value.\n",
    "1. A blue shaded area indicates level of confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data that we will plot\n",
    "pdp_goals = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=features, feature='var_82')\n",
    "\n",
    "# plot it\n",
    "pdp.pdp_plot(pdp_goals, 'var_82')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data that we will plot\n",
    "pdp_goals = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=features, feature='var_139')\n",
    "\n",
    "# plot it\n",
    "pdp.pdp_plot(pdp_goals, 'var_139')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data that we will plot\n",
    "pdp_goals = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=features, feature='var_110')\n",
    "\n",
    "# plot it\n",
    "pdp.pdp_plot(pdp_goals, 'var_110')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id=\"57\"></a> <br>\n",
    "## 5-7 SHAP Values\n",
    "**SHAP** (SHapley Additive exPlanations) is a unified approach to explain the output of **any machine learning model**. SHAP connects game theory with local explanations, uniting several previous methods [1-7] and representing the only possible consistent and locally accurate additive feature attribution method based on expectations (see the SHAP NIPS paper for details).\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/shap_diagram.png' width=400 height=400>\n",
    "[image credits](https://github.com/slundberg/shap)\n",
    "><font color=\"red\"><b>Note: </b></font>\n",
    "Shap can answer to this qeustion : **how the model works for an individual prediction?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_to_show = 5\n",
    "data_for_prediction = val_X.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired\n",
    "data_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n",
    "\n",
    "\n",
    "rfc_model.predict_proba(data_for_prediction_array);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap  # package used to calculate Shap values\n",
    "\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(rfc_model)\n",
    "\n",
    "# Calculate Shap values\n",
    "shap_values = explainer.shap_values(data_for_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If you look carefully at the code where we created the SHAP values, you'll notice we reference Trees in  **shap.TreeExplainer(my_model)**. But the SHAP package has explainers for every type of model.\n",
    "\n",
    "1. shap.DeepExplainer works with Deep Learning models.\n",
    "1. shap.KernelExplainer works with all models, though it is slower than other Explainers and it offers an approximation rather than exact Shap values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Shap values\n",
    "#shap_values = explainer.shap_values(val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
