{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init()\n",
    "import pixiedust\n",
    "import pyspark\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import Row\n",
    "from pyspark.sql.types import DateType\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np \n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Import `pyplot` \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the style to `ggplot`\n",
    "plt.style.use(\"ggplot\")\n",
    "pixiedust.enableJobMonitor()\n",
    "\n",
    "conf = SparkConf().setAppName(\"App\")\n",
    "conf = (conf.setMaster('local[*]')\n",
    "        .set('spark.executor.memory', '4G')\n",
    "        .set('spark.driver.memory', '30G')\n",
    "        .set('spark.driver.maxResultSize', '10G'))\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_key(row):\n",
    "    \n",
    "    sec = int(datetime.strptime(row['DTHR'], '%d/%m/%Y %H:%M:%S').second)\n",
    "    mi =  str(datetime.strptime(row['DTHR'], '%d/%m/%Y %H:%M:%S').minute)\n",
    "    hr =  str(datetime.strptime(row['DTHR'], '%d/%m/%Y %H:%M:%S').hour)\n",
    "    \n",
    "    partition = ''\n",
    "    if(sec <= 20):\n",
    "         partition = hr+'-'+mi+'-020'\n",
    "    elif(sec > 20 and sec <= 40):\n",
    "        partition = hr+'-'+mi+'-040'\n",
    "    else:\n",
    "        partition = hr+'-'+mi+'-060'\n",
    "    \n",
    "    key = row['COD_LINHA']+'-'+str(row['DATA'])+'-'+partition\n",
    "    return key\n",
    "\n",
    "\n",
    "def run_dbscan(df):\n",
    "    \n",
    "    key    = df[0]\n",
    "    values = df[1]\n",
    "    \n",
    "    d = [{'cod_linha':values[i]['COD_LINHA'], \\\n",
    "          'veic':values[i]['VEIC'],\\\n",
    "          'dthr':values[i]['DTHR'],\\\n",
    "          'lat':float(values[i]['LAT']),\\\n",
    "          'lon':float(values[i]['LON'])} for i in range(0,len(values))]\n",
    "    \n",
    "    coordinates = pd.DataFrame(d)\n",
    "    \n",
    "    db = DBSCAN(eps=1/6371., min_samples=3, algorithm='ball_tree', metric='haversine') \\\n",
    "         .fit_predict(np.radians(coordinates[['lat','lon']]))\n",
    "\n",
    "    coordinates['cluster_id'] = db\n",
    "    coordinates['key'] = key \n",
    "    return  coordinates.to_dict(\"records\")\n",
    "\n",
    "\n",
    "toDateTime =  udf(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S'), DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='../../../datascience/data/urbs/2018-11/19-23/'\n",
    "\n",
    "position_events = sqlContext.read.json(path+'*_veiculos.json')\n",
    "\n",
    "position_events = position_events.withColumn(\"DATA\", toDateTime(col('DTHR')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "tableView",
      "rowCount": "5",
      "table_noschema": "true",
      "table_nosearch": "true"
     }
    }
   },
   "outputs": [],
   "source": [
    "display(position_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linhas_kv = position_events.rdd.map(lambda x: (create_key(x), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_df = position_events.rdd                         \\\n",
    "    .map(lambda x: (create_key(x), x))                    \\\n",
    "    .map(lambda x: (x[0], [x[1]]))                        \\\n",
    "    .reduceByKey(lambda a, b: a + b)                      \\\n",
    "    .map(lambda x: run_dbscan(x))                         \\\n",
    "    .flatMap(lambda x: [item for item in x])              \\\n",
    "    .map(lambda l: Row(**dict(l))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "tableView",
      "table_noschema": "true",
      "table_nosearch": "true"
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(clusters_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_df.filter(\"cluster_id != -1\").repartition(1).write.mode('overwrite')       \\\n",
    "    .csv(\"/work/datascience/data/urbs-dbscan/\", sep=';',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixiedust-spark-2.4",
   "language": "python",
   "name": "pixiedustspark24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
